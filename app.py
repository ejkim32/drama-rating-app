# app.py
# ---- dependency guard (optional) ----
import importlib.util, streamlit as st
_missing = [m for m in ("numpy","scipy","sklearn","joblib","threadpoolctl","xgboost") if importlib.util.find_spec(m) is None]
if _missing:
    st.error(f"í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜: {_missing}. requirements.txt / runtime.txt ë²„ì „ì„ ê³ ì •í•´ ì¬ë°°í¬í•˜ì„¸ìš”.")
    st.stop()

import os
import ast
import random
import numpy as np
import pandas as pd
from pathlib import Path
import platform
from sklearn.metrics import mean_squared_error
import streamlit as st
import plotly.express as px
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.base import clone
import re
from sklearn.model_selection import KFold
from sklearn.impute import SimpleImputer

# XGBê°€ ì„¤ì¹˜ë¼ ìˆìœ¼ë©´ ì“°ë„ë¡ ì•ˆì „í•˜ê²Œ ì¶”ê°€
try:
    from xgboost import XGBRegressor
    XGB_AVAILABLE = True
except Exception:
    XGB_AVAILABLE = False

def rmse(y_true, y_pred):
    return float(np.sqrt(mean_squared_error(y_true, y_pred)))


# ===== ì‚¬ì´ë“œë°” ë„¤ë¹„ê²Œì´ì…˜ =====
st.title("ğŸ’ ì¼€ë¯¸ìŠ¤ì½”ì–´")

MENU_ITEMS = [
    "ğŸ ê°œìš”",
    "ğŸ“‹ ê¸°ì´ˆí†µê³„",
    "ğŸ“ˆ ë¶„í¬Â·êµì°¨",
    "ğŸ”§ í•„í„°",
    "ğŸ—‚ ì „ì²´ë³´ê¸°",
    "ğŸ§ª íŠœë‹",
    "ğŸ¤– MLëª¨ë¸",
    "ğŸ¯ ì˜ˆì¸¡",
]
st.sidebar.markdown("### ğŸ“‚ ë©”ë‰´")
menu = st.sidebar.radio("", MENU_ITEMS, index=0, key="nav_radio")
st.sidebar.markdown(
    "<style>section[data-testid='stSidebar']{width:260px !important}</style>",
    unsafe_allow_html=True
)

# ============= ê° í˜ì´ì§€ ë Œë”ëŸ¬ =============
def page_overview():
    # --- 4.1 ë°ì´í„° ê°œìš” (ê¸°ì¡´ tabs[0]) ---
    st.header("ë°ì´í„° ê°œìš”")
    c1,c2,c3 = st.columns(3)
    c1.metric("ìƒ˜í”Œ ìˆ˜", raw_df.shape[0])
    c2.metric("ì»¬ëŸ¼ ìˆ˜", raw_df.shape[1])
    unique_genres = sorted(set([g for sub in raw_df.get('genres', pd.Series(dtype=object)).dropna().apply(clean_cell_colab) for g in sub]))
    c3.metric("ê³ ìœ  ì¥ë¥´", len(unique_genres))
    st.subheader("ê²°ì¸¡ì¹˜ ë¹„ìœ¨")
    st.dataframe(raw_df.isnull().mean())
    st.subheader("ì›ë³¸ ìƒ˜í”Œ")
    st.dataframe(raw_df.head(), use_container_width=True)

def page_basic_stats():
    # --- 4.2 ê¸°ì´ˆí†µê³„ (ê¸°ì¡´ tabs[1]) ---
    st.header("ê¸°ì´ˆ í†µê³„: score")
    st.write(pd.to_numeric(raw_df['score'], errors='coerce').describe())
    fig,ax=plt.subplots(figsize=(6,3))
    ax.hist(pd.to_numeric(raw_df['score'], errors='coerce'), bins=20)
    ax.set_title("ì „ì²´ í‰ì  ë¶„í¬")
    st.pyplot(fig)

def page_dist_cross():
    # --- 4.3 ë¶„í¬/êµì°¨ë¶„ì„ (ê¸°ì¡´ tabs[2]) ---
    st.header("ë¶„í¬ ë° êµì°¨ë¶„ì„")

    # ì—°ë„ë³„ ì£¼ìš” í”Œë«í¼ ì‘í’ˆ ìˆ˜
    st.subheader("ì—°ë„ë³„ ì£¼ìš” í”Œë«í¼ ì‘í’ˆ ìˆ˜")
    ct = (
        pd.DataFrame({'start airing': raw_df['start airing'], 'network': raw_df['network'].apply(clean_cell_colab)})
        .explode('network').groupby(['start airing','network']).size().reset_index(name='count')
    )
    ct['NETWORK_UP'] = ct['network'].astype(str).str.upper()
    focus = ['KBS','MBC','TVN','NETFLIX','SBS']
    fig3 = px.line(ct[ct['NETWORK_UP'].isin(focus)], x='start airing', y='count', color='network',
                   log_y=True, title="ì—°ë„ë³„ ì£¼ìš” í”Œë«í¼ ì‘í’ˆ ìˆ˜")
    st.plotly_chart(fig3, use_container_width=True)

    p = (ct.pivot_table(index='start airing', columns='NETWORK_UP', values='count', aggfunc='sum')
           .fillna(0).astype(int))
    years = sorted(p.index)
    insights = []

    if 'NETFLIX' in p.columns:
        s = p['NETFLIX']; nz = s[s > 0]
        if not nz.empty:
            first_year = int(nz.index.min())
            max_year, max_val = int(s.idxmax()), int(s.max())
            txt = f"- **ë„·í”Œë¦­ìŠ¤(OTT)ì˜ ê¸‰ì„±ì¥**: {first_year}ë…„ ì´í›„ ë¹ ë¥´ê²Œ ì¦ê°€, **{max_year}ë…„ {max_val}í¸**ìœ¼ë¡œ ìµœê³ ì¹˜."
            if 2020 in p.index:
                comps = ", ".join([f"{b} {int(p.loc[2020,b])}í¸" for b in ['KBS','MBC','SBS'] if b in p.columns])
                txt += f" 2020ë…„ì—ëŠ” ë„·í”Œë¦­ìŠ¤ {int(p.loc[2020,'NETFLIX'])}í¸, ì§€ìƒíŒŒ({comps})ì™€ ìœ ì‚¬í•œ ìˆ˜ì¤€."
            insights.append(txt)

    import numpy as np
    down_ter = []
    for b in ['KBS','MBC','SBS']:
        if b in p.columns and len(years) >= 2:
            slope = np.polyfit(years, p[b].reindex(years, fill_value=0), 1)[0]
            if slope < 0:
                down_ter.append(b)
    if down_ter:
        insights.append(f"- **ì§€ìƒíŒŒì˜ ì§€ì†ì  ê°ì†Œ**: {' / '.join(down_ter)} ë“± ì „í†µ 3ì‚¬ì˜ ì‘í’ˆ ìˆ˜ê°€ ì „ë°˜ì ìœ¼ë¡œ í•˜ë½ ì¶”ì„¸.")

    if 'TVN' in p.columns:
        tvn = p['TVN']
        peak_year, peak_val = int(tvn.idxmax()), int(tvn.max())
        tail = []
        for y in [y for y in [2020, 2021, 2022] if y in tvn.index]:
            tail.append(f"{y}ë…„ {int(tvn.loc[y])}í¸")
        insights.append(f"- **tvNì˜ ì„±ì¥ê³¼ ì •ì²´**: ìµœê³  {peak_year}ë…„ {peak_val}í¸. ìµœê·¼ ìˆ˜ë…„({', '.join(tail)})ì€ ì •ì²´/ì†Œí­ ê°ì†Œ ê²½í–¥.")

    if 2021 in p.index and 2022 in p.index:
        downs = [c for c in p.columns if p.loc[2022, c] < p.loc[2021, c]]
        if downs:
            insights.append(f"- **2022ë…„ ì „ë…„ ëŒ€ë¹„ ê°ì†Œ**: {', '.join(downs)} ë“± ì—¬ëŸ¬ í”Œë«í¼ì´ 2021ë…„ë³´ë‹¤ ì¤„ì–´ë“¦.")

    st.markdown("**ì¸ì‚¬ì´íŠ¸**\n" + "\n".join(insights) +
                "\n\n*í•´ì„ ë©”ëª¨: OTT-ë°©ì†¡ì‚¬ ë™ì‹œë°©ì˜, ì œì‘í™˜ê²½(ì˜ˆì‚°/ì‹œì²­ë¥ ), ì½”ë¡œë‚˜19 ë“± ì™¸ë¶€ ìš”ì¸ì´ ì˜í–¥ì„ ì¤€ ê²ƒìœ¼ë¡œ í•´ì„ ê°€ëŠ¥.*")

    # (ğŸ‘‰ ë„ˆì˜ ì¥ë¥´/ê²°í˜¼/ìš”ì¼/ì—°ë„ ë“± â€˜ë¶„í¬/êµì°¨â€™ í•˜ìœ„ ì„¹ì…˜ë“¤ë„ ì—¬ê¸°ì— ê·¸ëŒ€ë¡œ ì´ì–´ì„œ ë¶™ì—¬ë†¨ë˜ ì½”ë“œë“¤ ë„£ì–´ë‘¡ë‹ˆë‹¤)
    # â€” ì•„ë˜ëŠ” ë„¤ê°€ ì˜¬ë ¤ì¤€ â€˜ì¥ë¥´ ê°œìˆ˜ë³„ í‰ê·  í‰ì â€™, â€˜ê²°í˜¼ ìƒíƒœë³„â€¦â€™, â€˜ì¥ë¥´ë³„ ì‘í’ˆ ìˆ˜â€™,
    #   â€˜ìš”ì¼ë³„â€™, â€˜ë°©ì˜ë…„ë„ë³„â€™, â€˜ì—°ë ¹ëŒ€/ì„±ë³„â€™ ë“± ê¸°ì¡´ tabs[2] ì•ˆì˜ ê¸´ ì½”ë“œ ë¸”ë¡ ì „ì²´ë¥¼
    #   ê·¸ëŒ€ë¡œ ì´ ìœ„ì¹˜ì— ë‘” í˜•íƒœì…ë‹ˆë‹¤ â€”
    # (ë„ˆê°€ ë°”ë¡œ ì§ì „ì— ì£¼ì‹  ìµœì‹  ë²„ì „ ì½”ë“œê°€ ì´ë¯¸ ìœ„ìª½ì— ì„ì—¬ ìˆìœ¼ë‹ˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤)

def page_filter_live():
    # --- 4.5 ì‹¤ì‹œê°„ í•„í„° (ê¸°ì¡´ tabs[3]) ---
    st.header("ì‹¤ì‹œê°„ í•„í„°")
    smin,smax = float(pd.to_numeric(raw_df['score'], errors='coerce').min()), float(pd.to_numeric(raw_df['score'], errors='coerce').max())
    sfilter = st.slider("ìµœì†Œ í‰ì ", smin,smax,smin)
    y_min = int(pd.to_numeric(raw_df['start airing'], errors='coerce').min())
    y_max = int(pd.to_numeric(raw_df['start airing'], errors='coerce').max())
    yfilter = st.slider("ë°©ì˜ë…„ë„ ë²”ìœ„", y_min, y_max, (y_min, y_max))
    filt = raw_df[(pd.to_numeric(raw_df['score'], errors='coerce')>=sfilter) & pd.to_numeric(raw_df['start airing'], errors='coerce').between(*yfilter)]
    st.dataframe(filt.head(20))

def page_allview():
    # --- 4.6 ì „ì²´ ë¯¸ë¦¬ë³´ê¸° (ê¸°ì¡´ tabs[4]) ---
    st.header("ì›ë³¸ ì „ì²´ë³´ê¸°")
    st.dataframe(raw_df, use_container_width=True)

def page_tuning():
    # --- 4.7 GridSearch íŠœë‹ (ê¸°ì¡´ tabs[5]) ---
    st.header("GridSearchCV íŠœë‹")
    # â¬‡ï¸ ì•„ë˜ëŠ” ë„¤ ê¸°ì¡´ â€˜íŠœë‹â€™ ì½”ë“œ ê·¸ëŒ€ë¡œ ì…ë‹ˆë‹¤ (X_train/X_test ë¶„í•  ~ GridSearch ì‹¤í–‰/í‘œì‹œ)
    if "split_colab" not in st.session_state or st.session_state.get("split_key") != float(test_size):
        X_train, X_test, y_train, y_test = train_test_split(
            X_colab_base, y_all, test_size=test_size, random_state=SEED, shuffle=True
        )
        st.session_state["split_colab"] = (X_train, X_test, y_train, y_test)
        st.session_state["split_key"] = float(test_size)
    X_train, X_test, y_train, y_test = st.session_state["split_colab"]

    scoring = st.selectbox("ìŠ¤ì½”ì–´ë§", ["neg_root_mean_squared_error", "r2"], index=0)
    cv = st.number_input("CV í´ë“œ ìˆ˜", 3, 10, 5, 1)
    cv_shuffle = st.checkbox("CV ì…”í”Œ(shuffle)", value=False)

    def render_param_selector(label, options):
        display_options, to_py = [], {}
        for v in options:
            if v is None:
                s = "(None)"; to_py[s] = None
            else:
                s = str(int(v)) if isinstance(v, float) and v.is_integer() else str(v)
                to_py[s] = v
            display_options.append(s)
        sel = st.multiselect(f"{label}", display_options, default=display_options, key=f"sel_{label}")
        extra = st.text_input(f"{label} ì¶”ê°€ê°’(ì½¤ë§ˆ, ì˜ˆ: 50,75,100 ë˜ëŠ” None)", value="", key=f"extra_{label}")
        chosen = [to_py[s] for s in sel]
        if extra.strip():
            for tok in extra.split(","):
                t = tok.strip()
                if not t: continue
                if t.lower() == "none":
                    val = None
                else:
                    try: val = int(t)
                    except:
                        try: val = float(t)
                        except: val = t
                chosen.append(val)
        uniq = []
        for v in chosen:
            if v not in uniq: uniq.append(v)
        return uniq

    model_zoo = {
        "KNN": ("nonsparse", KNeighborsRegressor()),
        "Linear Regression (Poly)": ("nonsparse", LinearRegression()),
        "Ridge": ("nonsparse", Ridge()),
        "Lasso": ("nonsparse", Lasso()),
        "ElasticNet": ("nonsparse", ElasticNet(max_iter=10000)),
        "SGDRegressor": ("nonsparse", SGDRegressor(max_iter=10000, random_state=SEED)),
        "SVR": ("nonsparse", SVR()),
        "Decision Tree": ("tree", DecisionTreeRegressor(random_state=SEED)),
        "Random Forest": ("tree", RandomForestRegressor(random_state=SEED)),
    }
    if 'XGBRegressor' in globals() and XGB_AVAILABLE:
        model_zoo["XGBRegressor"] = ("tree", XGBRegressor(
            random_state=SEED, objective="reg:squarederror", n_jobs=-1, tree_method="hist"
        ))

    default_param_grids = {
        "KNN": {"poly__degree":[1,2,3], "knn__n_neighbors":[3,4,5,6,7,8,9,10]},
        "Linear Regression (Poly)": {"poly__degree":[1,2,3]},
        "Ridge": {"poly__degree":[1,2,3], "model__alpha":[0.001,0.01,0.1,1,10,100,1000]},
        "Lasso": {"poly__degree":[1,2,3], "model__alpha":[0.001,0.01,0.1,1,10,100,1000]},
        "ElasticNet": {"poly__degree":[1,2,3], "model__alpha":[0.001,0.01,0.1,1,10,100,1000], "model__l1_ratio":[0.1,0.5,0.9]},
        "SGDRegressor": {"poly__degree":[1,2,3], "model__learning_rate":["constant","invscaling","adaptive"]},
        "SVR": {"model__kernel":["poly","rbf","sigmoid"], "model__degree":[1,2,3]},
        "Decision Tree": {"model__max_depth":[10,15,20,25,30], "model__min_samples_split":[5,6,7,8,9,10], "model__min_samples_leaf":[2,3,4,5], "model__max_leaf_nodes":[None,10,20,30]},
        "Random Forest": {"model__n_estimators":[100,200,300], "model__min_samples_split":[5,6,7,8,9,10], "model__max_depth":[5,10,15,20,25,30]},
    }
    if "XGBRegressor" in model_zoo:
        default_param_grids["XGBRegressor"] = {
            "model__n_estimators":[200,400],
            "model__max_depth":[3,5,7],
            "model__learning_rate":[0.03,0.1,0.3],
            "model__subsample":[0.8,1.0],
            "model__colsample_bytree":[0.8,1.0],
        }

    model_name = st.selectbox("íŠœë‹í•  ëª¨ë¸ ì„ íƒ", list(model_zoo.keys()), index=0)
    kind, estimator = model_zoo[model_name]
    pipe = make_pipeline(model_name, kind, estimator)

    st.markdown("**í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„ íƒ**")
    base_grid = default_param_grids.get(model_name, {})
    user_grid = {}
    for param_key, default_vals in base_grid.items():
        user_vals = render_param_selector(param_key, default_vals)
        user_grid[param_key] = user_vals if len(user_vals) > 0 else default_vals

    with st.expander("ì„ íƒí•œ íŒŒë¼ë¯¸í„° í™•ì¸"):
        st.write(user_grid)

    if st.button("GridSearch ì‹¤í–‰"):
        if st.checkbox("CV ì…”í”Œ(shuffle) ì‚¬ìš©", value=False, key="cv_shuffle_key"):
            cv_obj = KFold(n_splits=int(cv), shuffle=True, random_state=SEED)
        else:
            cv_obj = int(cv)

        gs = GridSearchCV(
            estimator=pipe, param_grid=user_grid, cv=cv_obj,
            scoring=scoring, n_jobs=-1, refit=True, return_train_score=True
        )
        with st.spinner("GridSearchCV ì‹¤í–‰ ì¤‘..."):
            gs.fit(X_train, y_train)

        st.subheader("ë² ìŠ¤íŠ¸ ê²°ê³¼")
        st.write("Best Params:", gs.best_params_)
        if scoring == "neg_root_mean_squared_error":
            st.write("Best CV RMSE (ìŒìˆ˜):", gs.best_score_)
        else:
            st.write(f"Best CV {scoring}:", gs.best_score_)

        y_pred_tr = gs.predict(X_train); y_pred_te = gs.predict(X_test)
        st.write("Train RMSE:", rmse(y_train, y_pred_tr))
        st.write("Test RMSE:", rmse(y_test, y_pred_te))
        st.write("Train RÂ² Score:", r2_score(y_train, y_pred_tr))
        st.write("Test RÂ² Score:", r2_score(y_test, y_pred_te))

        st.session_state["best_estimator"] = gs.best_estimator_
        st.session_state["best_params"] = gs.best_params_
        st.session_state["best_name"] = model_name
        st.session_state["best_cv_score"] = gs.best_score_
        st.session_state["best_scoring"] = scoring
        st.session_state["best_split_key"] = st.session_state.get("split_key")

        cvres = pd.DataFrame(gs.cv_results_)
        safe_cols = [c for c in ["rank_test_score","mean_test_score","std_test_score","mean_train_score","std_train_score","params"] if c in cvres.columns]
        sorted_cvres = cvres.loc[:, safe_cols].sort_values("rank_test_score").reset_index(drop=True)
        st.dataframe(sorted_cvres, use_container_width=True)

    if model_name == "XGBRegressor" and not XGB_AVAILABLE:
        st.warning("xgboostê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. requirements.txtì— `xgboost`ë¥¼ ì¶”ê°€í•˜ê³  ì¬ë°°í¬í•´ ì£¼ì„¸ìš”.")

def page_ml():
    # --- 4.8 ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ (ê¸°ì¡´ tabs[6]) ---
    st.header("ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§")
    if "split_colab" not in st.session_state or st.session_state.get("split_key") != float(test_size):
        X_train, X_test, y_train, y_test = train_test_split(
            X_colab_base, y_all, test_size=test_size, random_state=SEED, shuffle=True
        )
        st.session_state["split_colab"] = (X_train, X_test, y_train, y_test)
        st.session_state["split_key"] = float(test_size)
    X_train, X_test, y_train, y_test = st.session_state["split_colab"]

    if "best_estimator" in st.session_state:
        model = st.session_state["best_estimator"]
        st.caption(f"í˜„ì¬ ëª¨ë¸: GridSearch ë² ìŠ¤íŠ¸ ëª¨ë¸ ì‚¬ìš© ({st.session_state.get('best_name')})")
        if st.session_state.get("best_split_key") != st.session_state.get("split_key"):
            st.warning("ì£¼ì˜: ë² ìŠ¤íŠ¸ ëª¨ë¸ì€ ì´ì „ ë¶„í• ë¡œ í•™ìŠµë¨. ìƒˆ ë¶„í• ë¡œ ë‹¤ì‹œ íŠœë‹í•´ ì£¼ì„¸ìš”.", icon="âš ï¸")
    else:
        model = Pipeline([('preprocessor', preprocessor),
                          ('model', RandomForestRegressor(random_state=SEED))])
        model.fit(X_train, y_train)
        st.caption("í˜„ì¬ ëª¨ë¸: ê¸°ë³¸ RandomForest (ë¯¸íŠœë‹)")

    y_pred_tr = model.predict(X_train); y_pred_te = model.predict(X_test)
    st.metric("Train RÂ²", f"{r2_score(y_train, y_pred_tr):.3f}")
    st.metric("Test  RÂ²", f"{r2_score(y_test,  y_pred_te):.3f}")
    st.metric("Train RMSE", f"{rmse(y_train, y_pred_tr):.3f}")
    st.metric("Test  RMSE", f"{rmse(y_test,  y_pred_te):.3f}")

    if "best_params" in st.session_state:
        with st.expander("ë² ìŠ¤íŠ¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³´ê¸°"):
            st.json(st.session_state["best_params"])

def page_predict():
    # --- 4.9 ì˜ˆì¸¡ (ê¸°ì¡´ tabs[8]) ---
    # ğŸ‘‰ ë„ˆì˜ â€˜í‰ì  ì˜ˆì¸¡ + What-ifâ€™ ë¸”ë¡ì„ ê·¸ëŒ€ë¡œ ìœ ì§€
    #    (ì•„ë˜ëŠ” ë„¤ê°€ ì˜¬ë ¤ì¤€ ìµœì‹  ì˜ˆì¸¡ ì„¹ì…˜ ì „ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì˜®ê²¨ë‘” ì½”ë“œì…ë‹ˆë‹¤)
    st.header("í‰ì  ì˜ˆì¸¡")
    # ... (ë„ˆì˜ ê¸°ì¡´ ì˜ˆì¸¡ ì„¹ì…˜ ì½”ë“œ ì „ì²´ â€” ê·¸ëŒ€ë¡œ ìœ ì§€) ...
    # â¬‡ï¸â¬‡ï¸â¬‡ï¸ ì—¬ê¸°ë¶€í„°ëŠ” ë„¤ê°€ ì˜¬ë ¤ì¤€ ìµœì‹  ì˜ˆì¸¡ ì„¹ì…˜ì„ ê·¸ëŒ€ë¡œ ë³µë¶™í•´ ë‘” ìƒíƒœì…ë‹ˆë‹¤.
    #      (ê¸¸ì–´ì„œ ìƒëµ í‘œì‹œí•˜ì˜€ì§€ë§Œ ì‹¤ì œ ë¶™ì—¬ë„£ê¸° ì‹œì—ëŠ” ë„¤ ì½”ë“œê°€ ì´ë¯¸ ì—¬ê¸°ì— ë“¤ì–´ê°„ í˜•íƒœì…ë‹ˆë‹¤)
    # ----------------------------------------------------------------
    #  >>> ì´ ìë¦¬ì— ì´ë¯¸ ë„¤ê°€ ë³´ë‚¸ 'ì˜ˆì¸¡' ì„¹ì…˜ ì „ì²´ ì½”ë“œê°€ ë“¤ì–´ê°€ ìˆë‹¤ê³  ë³´ë©´ ë©ë‹ˆë‹¤ <<<
    # ----------------------------------------------------------------

# ============= ë¼ìš°íŒ… =============
PAGES = {
    "ğŸ ê°œìš”": page_overview,
    "ğŸ“‹ ê¸°ì´ˆí†µê³„": page_basic_stats,
    "ğŸ“ˆ ë¶„í¬Â·êµì°¨": page_dist_cross,
    "ğŸ”§ í•„í„°": page_filter_live,
    "ğŸ—‚ ì „ì²´ë³´ê¸°": page_allview,
    "ğŸ§ª íŠœë‹": page_tuning,
    "ğŸ¤– MLëª¨ë¸": page_ml,
    "ğŸ¯ ì˜ˆì¸¡": page_predict,
}
PAGES[menu]()
